from __future__ import print_function
import argparse
import torch
import torch.nn as nn
import pickle
from keras.preprocessing.sequence import pad_sequences
from keras.layers import *
import os
import torch.optim as optim
import sys
import random
import json

from models import LSTM, ESIM, BERT, BERT_snli, ROBERTA, EnsembleBERT
from dataset import *
from config import args
from BERT.tokenization import BertTokenizer
import time

# parallel training
from multi_train_utils.distributed_utils import init_distributed_mode, dist, is_main_process
from torch.utils.data import Dataset, DataLoader, SequentialSampler
import warnings
warnings.filterwarnings('ignore')

import os
# os.environ["CUDA_VISIBLE_DEVICES"] = '0'
# device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

def load_test_data():
    """Load data for test"""
    if args.task == 'snli':
        """ concate s1, s2 with '[SEP]/</s>' """
        test_x = []  # [[s1 '[SEP]' s2], [s1 '[SEP]' s2],...]
        for s1, s2 in zip(args.test_s1, args.test_s2):
            s = s1 + [args.sep_token] + s2
            test_x.append(s)
        test_y = args.test_labels
    else:
        test_x = args.datasets.test_seqs2
        test_x = [[args.inv_full_dict[w] for w in x] for x in test_x]
        test_y = args.datasets.test_y

    return test_x, test_y


def load_train_data():
    """Load data for training"""

    """Original train data"""
    if args.task == 'snli':
        """ concate s1, s2 with '[SEP]' """
        train_x = []  # [[s1 '[SEP]' s2], [s1 '[SEP]' s2],...]
        for s1, s2 in zip(args.train_s1, args.train_s2):
            s = s1 + [args.sep_token] + s2
            train_x.append(s)
        train_y = args.train_labels
    else:
        train_x = args.datasets.train_seqs2
        train_x = [[args.inv_full_dict[w] for w in x] for x in train_x]
        train_y = list(args.datasets.train_y)
    if is_main_process():
        print('#Train data:', len(train_y))


    """Adversarial augmentation"""
    """Load adversarial examples written in the file (generated by semPSO for top 10% train data)"""
    # # with open(args.data_path + '/%s/AD_dpso_sem_%s.pkl' % (args.task, args.target_model), 'rb') as f:
    # #     input_list, test_list, adv_y, adv_x, success, change_list, target_list = pickle.load(f)
    # # adv_x = [[args.inv_full_dict[w] for w in x] for x in adv_x]  # 网络输入是词语
    # """Load adversarial examples written in the file (generated by textfooler for top 25% train data)"""
    # train_x, train_y = [], []
    # adv_x, adv_y = [], []
    # # num_changed = []
    # adv_files = os.listdir(args.adv_path)
    # for file in adv_files:
    #     with open(args.adv_path + '/' + file, 'rb') as f:
    #         for line in f.readlines():
    #             js = json.loads(line.strip(), encoding='utf-8')
    #             adv_x.append(js['adv_texts'][0].split())
    #             adv_y.append(js['label'])
    #             # num_changed.append(js['num_changed'])
    # # num_changed = np.array(num_changed).mean()
    # # print('Average num changed:', num_changed)
    # # exit(0)
    # if is_main_process():
    #     print('#Adv examples: ', len(adv_y))
    # train_x.extend(adv_x)
    # train_y.extend(adv_y)
    # if is_main_process():
    #     print('#Final train data: ', len(train_y))

    # imdb打乱训练更好
    if args.task == 'imdb':
        c = list(zip(train_x, train_y))
        random.shuffle(c)
        train_x, train_y = zip(*c)
    return train_x, train_y


def eval_model(model, inputs_x, inputs_y):  # inputs_x is list of list with word
    model.eval()
    correct = 0.0
    if torch.cuda.device_count() > 1:
        predictor = model.module.text_pred()
    else:
        predictor = model.text_pred()
    # data_size = len(inputs_y)
    with torch.no_grad():
        outputs = predictor(inputs_x, inputs_x)
        pred = torch.argmax(outputs, dim=1)
        data_size = pred.shape[0]
        correct += torch.sum(torch.eq(pred, torch.LongTensor(inputs_y[:data_size]).cuda(args.rank)))
        acc = (correct.cpu().numpy())/float(data_size)
    return acc


def train_epoch(epoch, best_test, model, optimizer, dataloader_train, test_x, test_y, tokenizer):
    """Train for a epoch"""

    # Optimize
    time_start = time.time()
    model.train()

    # Eval for bert
    # if args.target_model == 'bert':
    #     if torch.cuda.device_count() > 1:
    #         if args.task == 'snli':
    #             model.module.model.eval()
    #         else:
    #             model.module.model.bert.eval()
    #     else:
    #         if args.task == 'snli':
    #             model.model.eval()
    #         else:
    #             model.model.bert.eval()

    criterion = nn.CrossEntropyLoss()
    cnt = 0
    for idx, (*train_x, train_y) in enumerate(dataloader_train):
        optimizer.zero_grad()
        input_ids, input_mask, segment_ids, _, train_y = \
            train_x[0].cuda(args.rank), train_x[1].cuda(args.rank), train_x[2].cuda(args.rank), train_x[3].cuda(
                args.rank), train_y.cuda(args.rank)

        # train_x[0], train_x[1], train_x[2], train_y = train_x[0].cuda(args.rank), train_x[1].cuda(args.rank),\
        #                                               train_x[2].cuda(args.rank), train_y.cuda(args.rank)
        cnt += 1
        if torch.cuda.device_count() > 1:
            output = model.module(train_x)
        else:
            output = model(train_x)
        loss = criterion(output, train_y)
        loss = loss.mean()
        loss.backward()
        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
    lr_decay = 0.8
    if lr_decay > 0:
        optimizer.param_groups[0]['lr'] *= lr_decay

    # For process 0
    if is_main_process() and epoch % 5 == 0:
        test_acc = eval_model(model, test_x, test_y)
        time_end = time.time()
        time_used = time_end - time_start
        sys.stdout.write("Epoch={} time={:.2f}s train_loss={:.6f} test_err={:.6f}(#{})\n".format(epoch, time_used, loss.item(), test_acc, len(test_y)))
        if test_acc > best_test:
            best_test = test_acc
            if torch.cuda.device_count() > 1:
                if args.target_model == 'bert':
                    if args.save_path:
                        if not os.path.exists(args.save_path):
                            os.makedirs(args.save_path)
                        torch.save(model.module.model.state_dict(), args.save_path + '/pytorch_model.bin')
                        model.module.model.config.to_json_file(args.save_path + '/bert_config.json')
                        # if args.task != 'snli':
                        #     tokenizer.save_vocabulary(args.save_path)
                        tokenizer.save_vocabulary(args.save_path)
                elif args.target_model == 'lstm':
                    torch.save(model.module.state_dict(), args.save_path)
            else:
                if args.target_model == 'bert':
                    if args.save_path:
                        if not os.path.exists(args.save_path):
                            os.makedirs(args.save_path)
                        torch.save(model.model.state_dict(), args.save_path + '/pytorch_model.bin')
                        model.model.config.to_json_file(args.save_path + '/bert_config.json')
                        # if args.task != 'snli':
                        #     tokenizer.save_vocabulary(args.save_path)
                        tokenizer.save_vocabulary(args.save_path)
                elif args.target_model == 'lstm':
                    torch.save(model.state_dict(), args.save_path)
            print('save model when test acc=', test_acc)

    # time_end = time.time()
    # time_used = time_end - time_start
    # 保存当前epoch
    if is_main_process():
        save_path = args.save_path + '_final'
        if save_path:
            if not os.path.exists(save_path): os.makedirs(save_path)
        if torch.cuda.device_count() > 1:
            torch.save(model.module.model.state_dict(), save_path + '/pytorch_model.bin')
            model.module.model.config.to_json_file(save_path + '/bert_config.json')
            tokenizer.save_vocabulary(save_path)
            print('Save final model when epoch=', epoch)
        else:
            torch.save(model.model.state_dict(), save_path + '/pytorch_model.bin')
            model.model.config.to_json_file(save_path + '/bert_config.json')
            tokenizer.save_vocabulary(save_path)
            print('Save final model when epoch=', epoch)

    dist.barrier()  # 这一句作用是：所有进程(gpu)上的代码都执行到这，才会执行该句下面的代码
    return best_test


def main(args):
    """Setting for parallel training"""
    if torch.cuda.is_available() is False:
        raise EnvironmentError("not find GPU device for training.")
    # args.rank = 0
    init_distributed_mode(args=args)
    torch.cuda.set_device(args.rank)

    """Load data"""
    test_x, test_y = load_test_data()
    train_x, train_y = load_train_data()

    # 随机选取1000个测试集作为验证集（从200个往后取，这样与攻击数据无重合）
    # if args.task in ['imdb', 'snli']:
    #     test_x, test_y = test_x[200:], test_y[200:]
    #     c = list(zip(test_x, test_y))
    #     random.shuffle(c)
    #     test_x, test_y = zip(*c)
    #     test_x, test_y = test_x[:1000], test_y[:1000]

    if args.target_model == 'lstm':
        if args.task == 'snli':
            model = ESIM(args).cuda(args.rank)
            """Load from trained model with different para name"""
            checkpoint = torch.load(args.target_model_path, map_location=args.device)
            checkpoint_new = {}
            for name, value in checkpoint.items():
                name = name.replace('enc_lstm.', '')
                checkpoint_new[name] = value
            model.load_state_dict(checkpoint_new)
            dataset_train = Dataset_LSTM_snli(args)
        else:
            model = LSTM(args).cuda()
            checkpoint = torch.load(args.target_model_path, map_location=args.device)
            model.load_state_dict(checkpoint)
            dataset_train = Dataset_LSTM(args)
    elif args.target_model == 'bert':
        # if args.task == 'snli':
        #     tokenizer = None
        #     model = BERT_snli(args).cuda(args.rank)
        #     # checkpoint = torch.load(args.target_model_path, map_location=args.device)
        #     # model.load_state_dict(checkpoint)
        # #     new_checkpointt = {}
        # #     # print(checkpoint.keys())
        # #     for name, param in checkpoint.items():
        # #         name = name.replace('model', 'model.bert')
        # #         name = name.replace('decoder', 'model.classifier')
        # #         new_checkpoint[name] = param
        # #     # 保存给bert
        # #     model = BERT(args).cuda(args.rank)
        # #     model.load_state_dict(new_checkpoint)
        # #     torch.save(model.model.state_dict(), args.save_path + '/pytorch_model.bin')
        # #     model.model.config.to_json_file(args.save_path + '/bert_config.json')
        # #     exit(0)
        #
        # else:
        #     tokenizer = BertTokenizer.from_pretrained(args.target_model_path, do_lower_case=True)  # 用来保存模型
        #     model = BERT(args).cuda(args.rank)
        #     # checkpoint = torch.load(args.target_model_path + '/pytorch_model.bin', map_location=device)
        #     # model.model.load_state_dict(checkpoint)
        tokenizer = BertTokenizer.from_pretrained(args.target_model_path, do_lower_case=True)  # 用来保存模型
        model = BERT(args).cuda(args.rank)
        dataset_train = Dataset_BERT(args)
    elif args.target_model == 'roberta':
        model = ROBERTA(args).cuda(args.rank)
        checkpoint = torch.load(args.target_model_path+'/pytorch_model.bin')
        model.load_state_dict(checkpoint)
        args.pad_token_id = model.encoder.config.pad_token_id
        tokenizer = None
        dataset_train = Dataset_ROBERTA(args)

    if torch.cuda.device_count() > 1:
        # model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)
        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.rank], find_unused_parameters=True)
        # model = model.module
    all_data_train, _ = dataset_train.transform_text(train_x, train_y)

    if torch.cuda.device_count() > 1:
        train_sampler = torch.utils.data.distributed.DistributedSampler(all_data_train)
        nw = min([os.cpu_count(), args.batch_size if args.batch_size > 1 else 0, 8])
        dataloader_train = DataLoader(all_data_train, sampler=train_sampler, pin_memory=True, num_workers=nw,
                                      batch_size=args.batch_size)  # batch_
    else:
        train_sampler = SequentialSampler(all_data_train)
        dataloader_train = DataLoader(all_data_train, sampler=train_sampler, batch_size=args.batch_size)

    # 对于bert，普通训练时，关闭除了分类层以外的权重
    # if args.target_model == 'bert':
    #     if torch.cuda.device_count() > 1:
    #         if args.task == 'snli':
    #             model.module.model.requires_grad_(False)  # snli
    #         else:
    #             model.module.model.bert.requires_grad_(False)  # mr/imdb
    #     else:
    #         if args.task == 'snli':
    #             model.model.requires_grad_(False)  # snli
    #         else:
    #             model.model.bert.requires_grad_(False)  # mr/imdb

    if is_main_process():
        test_acc = eval_model(model, test_x, test_y)
        print('Acc for test set is: {:.2%}(#{})'.format(test_acc, len(test_y)))
    exit(0)

    need_grad = lambda x: x.requires_grad
    optimizer = optim.Adam(filter(need_grad, model.parameters()), lr=args.lr)
    epoch = 200  # 10
    best_test = 0
    for e in range(epoch):
        if torch.cuda.device_count() > 1:
            train_sampler.set_epoch(epoch)
        best_test = train_epoch(e, best_test, model, optimizer, dataloader_train, test_x, test_y, tokenizer)

    # if is_main_process():
    #     test_acc = eval_model(model, test_x, test_y)
    #     print('Finally, acc for test set is: {:.2%}(#{})'.format(test_acc, len(test_y)))


if __name__ == '__main__':
    main(args)

    # with open('/pub/data/huangpei/PAT-AAAI23/TextFooler/adv_exps/train_set/tf_imdb_bert_success.pkl', 'rb') as fp:
    #     input_list, true_label_list, output_list, success, change_list, num_change_list, success_time = pickle.load(fp)
    #     output_list = [adv.split(' ') for adv in output_list]
    #     print(output_list[1])
